{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\johns\\anaconda3\\envs\\MIT_CGCNN\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--multi_adsorbate MULTI_ADSORBATE]\n",
      "                             [--data_dir DATA_DIR] [--run_mode RUN_MODE]\n",
      "                             [--split_ratio SPLIT_RATIO] [--epochs EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE] [--channels CHANNELS]\n",
      "                             [--seed SEED] [--save_model SAVE_MODEL]\n",
      "                             [--load_model LOAD_MODEL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\johns\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-25620rEmtuQMWzJkQ.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johns\\anaconda3\\envs\\MIT_CGCNN\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "# keras/sklearn libraries\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Reshape, BatchNormalization\n",
    "from keras.layers import (\n",
    "    Conv1D,\n",
    "    GlobalAveragePooling1D,\n",
    "    MaxPooling1D,\n",
    "    GlobalAveragePooling1D,\n",
    "    Reshape,\n",
    "    AveragePooling1D,\n",
    "    Flatten,\n",
    "    Concatenate,\n",
    ")\n",
    "from keras import backend\n",
    "from keras.callbacks import TensorBoard, LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"ML framework\")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--multi_adsorbate\",\n",
    "    default=0,\n",
    "    type=int,\n",
    "    help=\"train for single adsorbate (0) or multiple (1) (default: 0)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_dir\",\n",
    "    default=\"CH_data\",\n",
    "    type=str,\n",
    "    help=\"path to file containing DOS and targets (default: CH_data)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--run_mode\",\n",
    "    default=0,\n",
    "    type=int,\n",
    "    help=\"run regular (0) or 5-fold CV (1) (default: 0)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--split_ratio\", default=0.2, type=float, help=\"train/test ratio (default:0.2)\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--epochs\", default=60, type=int, help=\"number of total epochs to run (default:60)\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", default=32, type=int, help=\"batch size (default:32)\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--channels\", default=9, type=int, help=\"number of channels (default: 9)\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",\n",
    "    default=0,\n",
    "    type=int,\n",
    "    help=\"seed for data split(epochs), 0=random (default:0)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_model\",\n",
    "    default=0,\n",
    "    type=int,\n",
    "    help=\"path to file containing DOS and targets (default: 0)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--load_model\",\n",
    "    default=0,\n",
    "    type=int,\n",
    "    help=\"path to file containing DOS and targets (default: 0)\",\n",
    ")\n",
    "args = parser.parse_args(sys.argv[1:])\n",
    "\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # load data (replace with your own depending on the data format)\n",
    "    # Data format for x_surface_dos and x_adsorbate_dos is a numpy array with shape: (A, B, C) where A is number of samples, B is length of DOS file (2000), C is number of channels.\n",
    "    # Number of channels here is 27 for x_surface_dos which contains 9 orbitals x up to 3 adsorbing surface atoms. E.g. a top site will have the first 9 channels filled and remaining as zeros.\n",
    "    x_surface_dos, x_adsorbate_dos, y_targets = load_data(\n",
    "        args.multi_adsorbate, args.data_dir\n",
    "    )\n",
    "\n",
    "    if args.seed == 0:\n",
    "        args.seed = np.random.randint(1, 1e6)\n",
    "\n",
    "    if args.run_mode == 0:\n",
    "        run_training(args, x_surface_dos, x_adsorbate_dos, y_targets)\n",
    "    elif args.run_mode == 1:\n",
    "        run_kfold(args, x_surface_dos, x_adsorbate_dos, y_targets)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "def load_data(multi_adsorbate, data_dir):\n",
    "    ###load data containing: (1) dos of surface, (2) adsorption energy(target), (3) dos of adsorbate in gas phase (for multi-adsorbate)\n",
    "    if args.multi_adsorbate == 0:\n",
    "        with open(args.data_dir, \"rb\") as f:\n",
    "            surface_dos = pickle.load(f)\n",
    "            targets = pickle.load(f)\n",
    "        x_adsorbate_dos = []\n",
    "    elif args.multi_adsorbate == 1:\n",
    "        with open(args.data_dir, \"rb\") as f:\n",
    "            surface_dos = pickle.load(f)\n",
    "            targets = pickle.load(f)\n",
    "            x_adsorbate_dos = pickle.load(f)\n",
    "    ###Some data rearranging, depends on if atomic params are to be included as extra features in the DOS series or separately\n",
    "    ###entries 1700-2200 of the data are set to zero, these are states far above fermi level which seem to cause additional errors, reason being some states are not physically reasonable\n",
    "\n",
    "    ###First column is energy; not used in current implementation\n",
    "    surface_dos = surface_dos[:, 0:2000, 1:28]\n",
    "    ###States far above fermi level can be unphysical and set to zero\n",
    "    surface_dos[:, 1800:2000, 0:27] = 0\n",
    "    ###float32 is used for memory concerns\n",
    "    surface_dos = surface_dos.astype(np.float32)\n",
    "\n",
    "    if args.multi_adsorbate == 1:\n",
    "        x_adsorbate_dos = x_adsorbate_dos[:, 0:2000, 1:10]\n",
    "        x_adsorbate_dos = x_adsorbate_dos.astype(np.float32)\n",
    "\n",
    "    return surface_dos, x_adsorbate_dos, targets\n",
    "\n",
    "\n",
    "###Creates the ML model with keras\n",
    "###This is the overall model where all 3 adsorption sites are fitted at the same time\n",
    "def create_model(shared_conv, channels):\n",
    "\n",
    "    ###Each input represents one out of three possible bonding atoms\n",
    "    input1 = Input(shape=(2000, channels))\n",
    "    input2 = Input(shape=(2000, channels))\n",
    "    input3 = Input(shape=(2000, channels))\n",
    "\n",
    "    conv1 = shared_conv(input1)\n",
    "    conv2 = shared_conv(input2)\n",
    "    conv3 = shared_conv(input3)\n",
    "\n",
    "    convmerge = Concatenate(axis=-1)([conv1, conv2, conv3])\n",
    "    convmerge = Flatten()(convmerge)\n",
    "    convmerge = Dropout(0.2)(convmerge)\n",
    "    convmerge = Dense(200, activation=\"linear\")(convmerge)\n",
    "    convmerge = Dense(1000, activation=\"relu\")(convmerge)\n",
    "    convmerge = Dense(1000, activation=\"relu\")(convmerge)\n",
    "\n",
    "    out = Dense(1, activation=\"linear\")(convmerge)\n",
    "    # shared_conv.summary()\n",
    "    model = Model(input=[input1, input2, input3], output=out)\n",
    "    return model\n",
    "\n",
    "\n",
    "###This is the overall model where all 3 adsorption sites are fitted at the same time, and all adsorbates are fitted as well\n",
    "def create_model_combined(shared_conv, channels):\n",
    "\n",
    "    ###Each input represents one out of three possible bonding atoms\n",
    "    input1 = Input(shape=(2000, channels))\n",
    "    input2 = Input(shape=(2000, channels))\n",
    "    input3 = Input(shape=(2000, channels))\n",
    "    input4 = Input(shape=(2000, channels))\n",
    "\n",
    "    conv1 = shared_conv(input1)\n",
    "    conv2 = shared_conv(input2)\n",
    "    conv3 = shared_conv(input3)\n",
    "\n",
    "    adsorbate_conv = adsorbate_dos_featurizer(channels)\n",
    "    conv4 = adsorbate_conv(input4)\n",
    "\n",
    "    convmerge = Concatenate(axis=-1)([conv1, conv2, conv3, conv4])\n",
    "    convmerge = Flatten()(convmerge)\n",
    "    convmerge = Dropout(0.2)(convmerge)\n",
    "    convmerge = Dense(200, activation=\"linear\")(convmerge)\n",
    "    convmerge = Dense(1000, activation=\"relu\")(convmerge)\n",
    "    convmerge = Dense(1000, activation=\"relu\")(convmerge)\n",
    "\n",
    "    out = Dense(1, activation=\"linear\")(convmerge)\n",
    "\n",
    "    model = Model(input=[input1, input2, input3, input4], output=out)\n",
    "    return model\n",
    "\n",
    "\n",
    "###This sub-model is the convolutional network for the DOS\n",
    "###Uses the same model for each atom input channel\n",
    "###Input is a 2000 length DOS data series\n",
    "def dos_featurizer(channels):\n",
    "    input_dos = Input(shape=(2000, channels))\n",
    "    x1 = AveragePooling1D(pool_size=4, strides=4, padding=\"same\")(input_dos)\n",
    "    x2 = AveragePooling1D(pool_size=25, strides=4, padding=\"same\")(input_dos)\n",
    "    x3 = AveragePooling1D(pool_size=200, strides=4, padding=\"same\")(input_dos)\n",
    "    x = Concatenate(axis=-1)([x1, x2, x3])\n",
    "    x = Conv1D(50, 20, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(75, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = AveragePooling1D(pool_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = Conv1D(100, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = AveragePooling1D(pool_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = Conv1D(125, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = AveragePooling1D(pool_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = Conv1D(150, 3, activation=\"relu\", padding=\"same\", strides=1)(x)\n",
    "    shared_model = Model(input_dos, x)\n",
    "    return shared_model\n",
    "\n",
    "\n",
    "###Uses the same model for adsorbate but w/ separate weights\n",
    "def adsorbate_dos_featurizer(channels):\n",
    "    input_dos = Input(shape=(2000, channels))\n",
    "    x1 = AveragePooling1D(pool_size=4, strides=4, padding=\"same\")(input_dos)\n",
    "    x2 = AveragePooling1D(pool_size=25, strides=4, padding=\"same\")(input_dos)\n",
    "    x3 = AveragePooling1D(pool_size=200, strides=4, padding=\"same\")(input_dos)\n",
    "    x = Concatenate(axis=-1)([x1, x2, x3])\n",
    "    x = Conv1D(50, 20, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(75, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = AveragePooling1D(pool_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = Conv1D(100, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = AveragePooling1D(pool_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = Conv1D(125, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = AveragePooling1D(pool_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = Conv1D(150, 3, activation=\"relu\", padding=\"same\", strides=1)(x)\n",
    "    shared_model = Model(input_dos, x)\n",
    "    return shared_model\n",
    "\n",
    "\n",
    "###Simple learning rate scheduler\n",
    "def decay_schedule(epoch, lr):\n",
    "    if epoch == 0:\n",
    "        lr = 0.001\n",
    "    elif epoch == 15:\n",
    "        lr = 0.0005\n",
    "    elif epoch == 35:\n",
    "        lr = 0.0001\n",
    "    elif epoch == 45:\n",
    "        lr = 0.00005\n",
    "    elif epoch == 55:\n",
    "        lr = 0.00001\n",
    "    return lr\n",
    "\n",
    "\n",
    "# regular training\n",
    "def run_training(args, x_surface_dos, x_adsorbate_dos, y_targets):\n",
    "\n",
    "    ###Split data into train and test\n",
    "    if args.multi_adsorbate == 0:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            x_surface_dos, y_targets, test_size=args.split_ratio, random_state=88\n",
    "        )\n",
    "    elif args.multi_adsorbate == 1:\n",
    "        x_train, x_test, y_train, y_test, ads_train, ads_test = train_test_split(\n",
    "            x_surface_dos,\n",
    "            y_targets,\n",
    "            x_adsorbate_dos,\n",
    "            test_size=args.split_ratio,\n",
    "            random_state=88,\n",
    "        )\n",
    "    ###Scaling data\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train.reshape(-1, x_train.shape[2])).reshape(\n",
    "        x_train.shape\n",
    "    )\n",
    "    x_test = scaler.transform(x_test.reshape(-1, x_test.shape[2])).reshape(x_test.shape)\n",
    "\n",
    "    if args.multi_adsorbate == 1:\n",
    "        ads_train = scaler.fit_transform(\n",
    "            ads_train.reshape(-1, ads_train.shape[2])\n",
    "        ).reshape(ads_train.shape)\n",
    "        ads_test = scaler.transform(ads_test.reshape(-1, ads_test.shape[2])).reshape(\n",
    "            ads_test.shape\n",
    "        )\n",
    "\n",
    "    ###call and fit model\n",
    "    shared_conv = dos_featurizer(args.channels)\n",
    "    lr_scheduler = LearningRateScheduler(decay_schedule, verbose=0)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time.time()), histogram_freq=1)\n",
    "\n",
    "    ###FOr testing purposes, a model where 3 adsorption sites fitted simultaneously and 3 separately are done by comparison\n",
    "    if args.multi_adsorbate == 0:\n",
    "        if args.load_model == 0:\n",
    "            model = create_model(shared_conv, args.channels)\n",
    "            model.compile(\n",
    "                loss=\"logcosh\", optimizer=Adam(0.001), metrics=[\"mean_absolute_error\"]\n",
    "            )\n",
    "        elif args.load_model == 1:\n",
    "            print(\"Loading model...\")\n",
    "            model = load_model(\"DOSnet_saved.h5\", compile=False)\n",
    "            model.compile(\n",
    "                loss=\"logcosh\", optimizer=Adam(0.001), metrics=[\"mean_absolute_error\"]\n",
    "            )\n",
    "        model.summary()\n",
    "        model.fit(\n",
    "            [x_train[:, :, 0:9], x_train[:, :, 9:18], x_train[:, :, 18:27]],\n",
    "            y_train,\n",
    "            batch_size=args.batch_size,\n",
    "            epochs=args.epochs,\n",
    "            validation_data=(\n",
    "                [x_test[:, :, 0:9], x_test[:, :, 9:18], x_test[:, :, 18:27]],\n",
    "                y_test,\n",
    "            ),\n",
    "            callbacks=[tensorboard, lr_scheduler],\n",
    "        )\n",
    "        train_out = model.predict(\n",
    "            [x_train[:, :, 0:9], x_train[:, :, 9:18], x_train[:, :, 18:27]]\n",
    "        )\n",
    "        train_out = train_out.reshape(len(train_out))\n",
    "        test_out = model.predict(\n",
    "            [x_test[:, :, 0:9], x_test[:, :, 9:18], x_test[:, :, 18:27]]\n",
    "        )\n",
    "        test_out = test_out.reshape(len(test_out))\n",
    "\n",
    "    elif args.multi_adsorbate == 1:\n",
    "        model = create_model_combined(shared_conv, args.channels)\n",
    "        model.compile(\n",
    "            loss=\"logcosh\", optimizer=Adam(0.001), metrics=[\"mean_absolute_error\"]\n",
    "        )\n",
    "        model.summary()\n",
    "        model.fit(\n",
    "            [x_train[:, :, 0:9], x_train[:, :, 9:18], x_train[:, :, 18:27], ads_train],\n",
    "            y_train,\n",
    "            batch_size=args.batch_size,\n",
    "            epochs=args.epochs,\n",
    "            validation_data=(\n",
    "                [x_test[:, :, 0:9], x_test[:, :, 9:18], x_test[:, :, 18:27], ads_test],\n",
    "                y_test,\n",
    "            ),\n",
    "            callbacks=[tensorboard, lr_scheduler],\n",
    "        )\n",
    "        train_out = model.predict(\n",
    "            [x_train[:, :, 0:9], x_train[:, :, 9:18], x_train[:, :, 18:27], ads_train]\n",
    "        )\n",
    "        train_out = train_out.reshape(len(train_out))\n",
    "        test_out = model.predict(\n",
    "            [x_test[:, :, 0:9], x_test[:, :, 9:18], x_test[:, :, 18:27], ads_test]\n",
    "        )\n",
    "        test_out = test_out.reshape(len(test_out))\n",
    "\n",
    "    ###this is just to write the results to a file\n",
    "    print(\"train MAE: \", mean_absolute_error(y_train, train_out))\n",
    "    print(\"train RMSE: \", mean_squared_error(y_train, train_out) ** (0.5))\n",
    "    print(\"test MAE: \", mean_absolute_error(y_test, test_out))\n",
    "    print(\"test RMSE: \", mean_squared_error(y_test, test_out) ** (0.5))\n",
    "\n",
    "    with open(\"predict_train.txt\", \"w\") as f:\n",
    "        np.savetxt(f, np.stack((y_train, train_out), axis=-1))\n",
    "    with open(\"predict_test.txt\", \"w\") as f:\n",
    "        np.savetxt(f, np.stack((y_test, test_out), axis=-1))\n",
    "\n",
    "    if args.save_model == 1:\n",
    "        print(\"Saving model...\")\n",
    "        model.save(\"DOSnet_saved.h5\")\n",
    "\n",
    "\n",
    "# kfold\n",
    "def run_kfold(args, x_surface_dos, x_adsorbate_dos, y_targets):\n",
    "    cvscores = []\n",
    "    count = 0\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=args.seed)\n",
    "\n",
    "    for train, test in kfold.split(x_surface_dos, y_targets):\n",
    "\n",
    "        scaler_CV = StandardScaler()\n",
    "        x_surface_dos[train, :, :] = scaler_CV.fit_transform(\n",
    "            x_surface_dos[train, :, :].reshape(-1, x_surface_dos[train, :, :].shape[-1])\n",
    "        ).reshape(x_surface_dos[train, :, :].shape)\n",
    "        x_surface_dos[test, :, :] = scaler_CV.transform(\n",
    "            x_surface_dos[test, :, :].reshape(-1, x_surface_dos[test, :, :].shape[-1])\n",
    "        ).reshape(x_surface_dos[test, :, :].shape)\n",
    "        if args.multi_adsorbate == 1:\n",
    "            x_adsorbate_dos[train, :, :] = scaler.fit_transform(\n",
    "                x_adsorbate_dos[train, :, :].reshape(\n",
    "                    -1, x_adsorbate_dos[train, :, :].shape[-1]\n",
    "                )\n",
    "            ).reshape(x_adsorbate_dos[train, :, :].shape)\n",
    "            x_adsorbate_dos[test, :, :] = scaler.transform(\n",
    "                x_adsorbate_dos[test, :, :].reshape(\n",
    "                    -1, x_adsorbate_dos[test, :, :].shape[-1]\n",
    "                )\n",
    "            ).reshape(x_adsorbate_dos[test, :, :].shape)\n",
    "\n",
    "        keras.backend.clear_session()\n",
    "        shared_conv = dos_featurizer(args.channels)\n",
    "        lr_scheduler = LearningRateScheduler(decay_schedule, verbose=0)\n",
    "        if args.multi_adsorbate == 0:\n",
    "            model_CV = create_model(shared_conv, args.channels)\n",
    "            model_CV.compile(\n",
    "                loss=\"logcosh\", optimizer=Adam(0.001), metrics=[\"mean_absolute_error\"]\n",
    "            )\n",
    "            model_CV.fit(\n",
    "                [\n",
    "                    x_surface_dos[train, :, 0:9],\n",
    "                    x_surface_dos[train, :, 9:18],\n",
    "                    x_surface_dos[train, :, 18:27],\n",
    "                ],\n",
    "                y_targets[train],\n",
    "                batch_size=args.batch_size,\n",
    "                epochs=args.epochs,\n",
    "                verbose=0,\n",
    "                callbacks=[lr_scheduler],\n",
    "            )\n",
    "            scores = model_CV.evaluate(\n",
    "                [\n",
    "                    x_surface_dos[test, :, 0:9],\n",
    "                    x_surface_dos[test, :, 9:18],\n",
    "                    x_surface_dos[test, :, 18:27],\n",
    "                ],\n",
    "                y_targets[test],\n",
    "                verbose=0,\n",
    "            )\n",
    "            train_out_CV_temp = model_CV.predict(\n",
    "                [\n",
    "                    x_surface_dos[test, :, 0:9],\n",
    "                    x_surface_dos[test, :, 9:18],\n",
    "                    x_surface_dos[test, :, 18:27],\n",
    "                ]\n",
    "            )\n",
    "            train_out_CV_temp = train_out_CV_temp.reshape(len(train_out_CV_temp))\n",
    "        elif args.multi_adsorbate == 1:\n",
    "            model_CV = create_model_combined(shared_conv, args.channels)\n",
    "            model_CV.compile(\n",
    "                loss=\"logcosh\", optimizer=Adam(0.001), metrics=[\"mean_absolute_error\"]\n",
    "            )\n",
    "            model_CV.fit(\n",
    "                [\n",
    "                    x_surface_dos[train, :, 0:9],\n",
    "                    x_surface_dos[train, :, 9:18],\n",
    "                    x_surface_dos[train, :, 18:27],\n",
    "                    x_adsorbate_dos[train, :, :],\n",
    "                ],\n",
    "                y_targets[train],\n",
    "                batch_size=args.batch_size,\n",
    "                epochs=args.epochs,\n",
    "                verbose=0,\n",
    "                callbacks=[lr_scheduler],\n",
    "            )\n",
    "            scores = model_CV.evaluate(\n",
    "                [\n",
    "                    x_surface_dos[test, :, 0:9],\n",
    "                    x_surface_dos[test, :, 9:18],\n",
    "                    x_surface_dos[test, :, 18:27],\n",
    "                    x_adsorbate_dos[test, :, :],\n",
    "                ],\n",
    "                y_targets[test],\n",
    "                verbose=0,\n",
    "            )\n",
    "            train_out_CV_temp = model_CV.predict(\n",
    "                [\n",
    "                    x_surface_dos[test, :, 0:9],\n",
    "                    x_surface_dos[test, :, 9:18],\n",
    "                    x_surface_dos[test, :, 18:27],\n",
    "                    x_adsorbate_dos[test, :, :],\n",
    "                ]\n",
    "            )\n",
    "            train_out_CV_temp = train_out_CV_temp.reshape(len(train_out_CV_temp))\n",
    "        print((model_CV.metrics_names[1], scores[1]))\n",
    "        cvscores.append(scores[1])\n",
    "        if count == 0:\n",
    "            train_out_CV = train_out_CV_temp\n",
    "            test_y_CV = y_targets[test]\n",
    "            test_index = test\n",
    "        elif count > 0:\n",
    "            train_out_CV = np.append(train_out_CV, train_out_CV_temp)\n",
    "            test_y_CV = np.append(test_y_CV, y_targets[test])\n",
    "            test_index = np.append(test_index, test)\n",
    "        count = count + 1\n",
    "    print((np.mean(cvscores), np.std(cvscores)))\n",
    "    print(len(test_y_CV))\n",
    "    print(len(train_out_CV))\n",
    "    with open(\"CV_predict.txt\", \"w\") as f:\n",
    "        np.savetxt(f, np.stack((test_y_CV, train_out_CV), axis=-1))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MIT_CGCNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
