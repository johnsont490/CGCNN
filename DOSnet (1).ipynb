{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4601a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\\\Users\\\\johns\\\\Downloads\\\\trainingdata.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95654468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Reshape, BatchNormalization\n",
    "from keras.layers import (\n",
    "    Conv1D,\n",
    "    GlobalAveragePooling1D,\n",
    "    MaxPooling1D,\n",
    "    GlobalAveragePooling1D,\n",
    "    Reshape,\n",
    "    AveragePooling1D,\n",
    "    Flatten,\n",
    "    Concatenate,\n",
    ")\n",
    "from keras import backend\n",
    "from keras.callbacks import TensorBoard, LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "multi_adsorbate = 0  \n",
    "data_dir = \"C:\\\\Users\\\\johns\\\\Downloads\\\\trainingdata.pkl\"  \n",
    "run_mode = 0  # 0 for regular, 1 for 5-fold CV\n",
    "split_ratio = 0.2\n",
    "epochs = 60\n",
    "batch_size = 32\n",
    "channels = 2\n",
    "seed = np.random.randint(1, 1e6)\n",
    "save_model = True  \n",
    "load_model_path = 0  \n",
    "load_model = 0\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    x_surface_dos, x_adsorbate_dos, y_targets = load_data(\n",
    "        multi_adsorbate, data_dir\n",
    "    )\n",
    "\n",
    "    if run_mode == 0:\n",
    "        run_training(x_surface_dos, x_adsorbate_dos, y_targets)\n",
    "    elif run_mode == 1:\n",
    "        run_kfold(x_surface_dos, x_adsorbate_dos, y_targets)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "def load_data(multi_adsorbate, data_dir):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        loaded_data = pickle.load(file)\n",
    "\n",
    "    target_training = loaded_data['targetd_train']\n",
    "    features  = loaded_data['featured_train '] \n",
    "    DOS_training = [ [list(pair) for pair in zip(*item)] for item in features]\n",
    "    DOS_feat_training = np.array(DOS_training)\n",
    "    targets = np.array(target_training)\n",
    "    surface_dos = DOS_feat_training.astype(np.float32)\n",
    "    print(targets,surface_dos)\n",
    "    return surface_dos, None, targets\n",
    "\n",
    "\n",
    "### changed to take one input instead of three \n",
    "def create_model(channels):\n",
    "    input_layer = Input(shape=(2000, channels))\n",
    "    x = Conv1D(64, 3, activation='relu', padding='same')(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output_layer = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# altered \n",
    "def create_model_combined(shared_conv, channels):\n",
    "    ###Each input represents one out of three possible bonding atoms\n",
    "    input1 = Input(shape=(2000, channels))\n",
    "    input2 = Input(shape=(2000, channels))\n",
    "    input3 = Input(shape=(2000, channels))\n",
    "    input4 = Input(shape=(2000, channels))\n",
    "\n",
    "    conv1 = shared_conv(input1)\n",
    "    conv2 = shared_conv(input2)\n",
    "    conv3 = shared_conv(input3)\n",
    "\n",
    "    adsorbate_conv = adsorbate_dos_featurizer(channels)\n",
    "    conv4 = adsorbate_conv(input4)\n",
    "\n",
    "    convmerge = Concatenate(axis=-1)([conv1, conv2, conv3, conv4])\n",
    "    convmerge = Flatten()(convmerge)\n",
    "    convmerge = Dropout(0.2)(convmerge)\n",
    "    convmerge = Dense(400, activation=\"linear\")(convmerge) # change \n",
    "    convmerge = Dense(1000, activation=\"relu\")(convmerge)\n",
    "    convmerge = Dense(1000, activation=\"relu\")(convmerge)\n",
    "\n",
    "    out = Dense(1, activation=\"linear\")(convmerge)\n",
    "    model = Model(inputs=[input1, input2, input3, input4], outputs=out)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "###This sub-model is the convolutional network for the DOS\n",
    "###Input is a 2000 length DOS data series\n",
    "def dos_featurizer(channels):\n",
    "    input_dos = Input(shape=(2000, channels))\n",
    "    x1 = AveragePooling1D(pool_size=4, strides=4, padding=\"same\")(input_dos)\n",
    "    x2 = AveragePooling1D(pool_size=25, strides=4, padding=\"same\")(input_dos)\n",
    "    x3 = AveragePooling1D(pool_size=200, strides=4, padding=\"same\")(input_dos)\n",
    "    x = Concatenate(axis=-1)([x1, x2, x3])\n",
    "    x = Conv1D(50, 20, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(75, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = AveragePooling1D(pool_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = Conv1D(100, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = AveragePooling1D(pool_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = Conv1D(125, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = AveragePooling1D(pool_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = Conv1D(150, 3, activation=\"relu\", padding=\"same\", strides=1)(x)\n",
    "    shared_model = Model(input_dos, x)\n",
    "    return shared_model\n",
    "\n",
    "\n",
    "###Uses the same model for adsorbate but w/ separate weights\n",
    "def adsorbate_dos_featurizer(channels):\n",
    "    input_dos = Input(shape=(2000, channels))\n",
    "    x1 = AveragePooling1D(pool_size=4, strides=4, padding=\"same\")(input_dos)\n",
    "    x2 = AveragePooling1D(pool_size=25, strides=4, padding=\"same\")(input_dos)\n",
    "    x3 = AveragePooling1D(pool_size=200, strides=4, padding=\"same\")(input_dos)\n",
    "    x = Concatenate(axis=-1)([x1, x2, x3])\n",
    "    x = Conv1D(50, 20, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(75, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = AveragePooling1D(pool_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = Conv1D(100, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = AveragePooling1D(pool_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = Conv1D(125, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "    x = AveragePooling1D(pool_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = Conv1D(150, 3, activation=\"relu\", padding=\"same\", strides=1)(x)\n",
    "    shared_model = Model(input_dos, x)\n",
    "    return shared_model\n",
    "\n",
    "\n",
    "###Simple learning rate scheduler\n",
    "def decay_schedule(epoch, lr):\n",
    "    if epoch == 0:\n",
    "        lr = 0.001\n",
    "    elif epoch == 15:\n",
    "        lr = 0.0005\n",
    "    elif epoch == 35:\n",
    "        lr = 0.0001\n",
    "    elif epoch == 45:\n",
    "        lr = 0.00005\n",
    "    elif epoch == 55:\n",
    "        lr = 0.00001\n",
    "    return lr\n",
    "\n",
    "def run_training(x_surface_dos, x_adsorbate_dos, y_targets):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_surface_dos, y_targets, test_size=split_ratio, random_state=88\n",
    "    )\n",
    "\n",
    "    # Scaling data\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train.reshape(-1, x_train.shape[-1])).reshape(x_train.shape)\n",
    "    x_test = scaler.transform(x_test.reshape(-1, x_test.shape[-1])).reshape(x_test.shape)\n",
    "\n",
    "    # Call and fit model\n",
    "    model = create_model(channels)\n",
    "    model.compile(loss=\"logcosh\", optimizer=Adam(0.001), metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(decay_schedule, verbose=0)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time.time()), histogram_freq=1)\n",
    "\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[tensorboard, lr_scheduler]\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    train_out = model.predict(x_train)\n",
    "    test_out = model.predict(x_test)\n",
    "\n",
    "    print(\"train MAE: \", mean_absolute_error(y_train, train_out))\n",
    "    print(\"train RMSE: \", np.sqrt(mean_squared_error(y_train, train_out)))\n",
    "    print(\"test MAE: \", mean_absolute_error(y_test, test_out))\n",
    "    print(\"test RMSE: \", np.sqrt(mean_squared_error(y_test, test_out)))\n",
    "\n",
    "    if save_model:\n",
    "        model.save(\"DOSnet_simplified.h5\")\n",
    "\n",
    "\n",
    "# kfold\n",
    "def run_kfold(x_surface_dos, x_adsorbate_dos, y_targets):\n",
    "    cvscores = []\n",
    "    count = 0\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    for train, test in kfold.split(x_surface_dos, y_targets):\n",
    "\n",
    "        scaler_CV = StandardScaler()\n",
    "        x_surface_dos[train, :, :] = scaler_CV.fit_transform(\n",
    "            x_surface_dos[train, :, :].reshape(-1, x_surface_dos[train, :, :].shape[-1])\n",
    "        ).reshape(x_surface_dos[train, :, :].shape)\n",
    "        x_surface_dos[test, :, :] = scaler_CV.transform(\n",
    "            x_surface_dos[test, :, :].reshape(-1, x_surface_dos[test, :, :].shape[-1])\n",
    "        ).reshape(x_surface_dos[test, :, :].shape)\n",
    "        if multi_adsorbate == 1:\n",
    "            x_adsorbate_dos[train, :, :] = scaler.fit_transform(\n",
    "                x_adsorbate_dos[train, :, :].reshape(\n",
    "                    -1, x_adsorbate_dos[train, :, :].shape[-1]\n",
    "                )\n",
    "            ).reshape(x_adsorbate_dos[train, :, :].shape)\n",
    "            x_adsorbate_dos[test, :, :] = scaler.transform(\n",
    "                x_adsorbate_dos[test, :, :].reshape(\n",
    "                    -1, x_adsorbate_dos[test, :, :].shape[-1]\n",
    "                )\n",
    "            ).reshape(x_adsorbate_dos[test, :, :].shape)\n",
    "\n",
    "        keras.backend.clear_session()\n",
    "        shared_conv = dos_featurizer(channels)\n",
    "        lr_scheduler = LearningRateScheduler(decay_schedule, verbose=0)\n",
    "        if multi_adsorbate == 0:\n",
    "            model_CV = create_model(shared_conv, channels)\n",
    "            model_CV.compile(\n",
    "                loss=\"logcosh\", optimizer=Adam(0.001), metrics=[\"mean_absolute_error\"]\n",
    "            )\n",
    "            model_CV.fit(\n",
    "                [\n",
    "                    x_surface_dos,\n",
    "                ],\n",
    "                y_targets[train],\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=0,\n",
    "                callbacks=[lr_scheduler],\n",
    "            )\n",
    "            scores = model_CV.evaluate(\n",
    "                [\n",
    "                    x_surface_dos,\n",
    "                ],\n",
    "                y_targets[test],\n",
    "                verbose=0,\n",
    "            )\n",
    "            train_out_CV_temp = model_CV.predict(\n",
    "                [\n",
    "                    x_surface_dos,\n",
    "                ]\n",
    "            )\n",
    "            train_out_CV_temp = train_out_CV_temp.reshape(len(train_out_CV_temp))\n",
    "        elif multi_adsorbate == 1:\n",
    "            model_CV = create_model_combined(shared_conv, channels)\n",
    "            model_CV.compile(\n",
    "                loss=\"logcosh\", optimizer=Adam(0.001), metrics=[\"mean_absolute_error\"]\n",
    "            )\n",
    "            model_CV.fit(\n",
    "                [\n",
    "                    x_surface_dos,\n",
    "                    x_adsorbate_dos[train, :, :],\n",
    "                ],\n",
    "                y_targets[train],\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                verbose=0,\n",
    "                callbacks=[lr_scheduler],\n",
    "            )\n",
    "            scores = model_CV.evaluate(\n",
    "                [\n",
    "                    x_surface_dos,\n",
    "                    x_adsorbate_dos[test, :, :],\n",
    "                ],\n",
    "                y_targets[test],\n",
    "                verbose=0,\n",
    "            )\n",
    "            train_out_CV_temp = model_CV.predict(\n",
    "                [\n",
    "                    x_surface_dos,\n",
    "                    x_adsorbate_dos[test, :, :],\n",
    "                ]\n",
    "            )\n",
    "            train_out_CV_temp = train_out_CV_temp.reshape(len(train_out_CV_temp))\n",
    "        print((model_CV.metrics_names[1], scores[1]))\n",
    "        cvscores.append(scores[1])\n",
    "        if count == 0:\n",
    "            train_out_CV = train_out_CV_temp\n",
    "            test_y_CV = y_targets[test]\n",
    "            test_index = test\n",
    "        elif count > 0:\n",
    "            train_out_CV = np.append(train_out_CV, train_out_CV_temp)\n",
    "            test_y_CV = np.append(test_y_CV, y_targets[test])\n",
    "            test_index = np.append(test_index, test)\n",
    "        count = count + 1\n",
    "    print((np.mean(cvscores), np.std(cvscores)))\n",
    "    print(len(test_y_CV))\n",
    "    print(len(train_out_CV))\n",
    "    with open(\"CV_predict.txt\", \"w\") as f:\n",
    "        np.savetxt(f, np.stack((test_y_CV, train_out_CV), axis=-1))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
